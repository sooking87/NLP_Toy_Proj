{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>neu_sentiment</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>com_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>textblob_pol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>frankie laine</td>\n",
       "      <td>i believe</td>\n",
       "      <td>1950</td>\n",
       "      <td>I Believe Lyrics\\nI believe for every drop of ...</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.7506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>johnnie ray</td>\n",
       "      <td>cry</td>\n",
       "      <td>1950</td>\n",
       "      <td>Cry Lyrics\\nOoh-wah\\nOoh-wah\\nOoh-wah\\n\\n\\nIf ...</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>p?rez prado</td>\n",
       "      <td>patricia</td>\n",
       "      <td>1950</td>\n",
       "      <td>Patricia LyricsKiss her and your lips will alw...</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>lefty frizzell</td>\n",
       "      <td>if you've got the money i've got the time</td>\n",
       "      <td>1950</td>\n",
       "      <td>If You’ve Got The Money I’ve Got The Time Lyri...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lefty frizzell</td>\n",
       "      <td>i want to be with you always</td>\n",
       "      <td>1950</td>\n",
       "      <td>I Want To Be With You Always Lyrics\\nI lose my...</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21739</th>\n",
       "      <td>21739</td>\n",
       "      <td>mack 10</td>\n",
       "      <td>10 million ways</td>\n",
       "      <td>2019</td>\n",
       "      <td>10 Million Ways Lyrics\\nCause if you fuck with...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.9936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21740</th>\n",
       "      <td>21740</td>\n",
       "      <td>m.o.p.</td>\n",
       "      <td>ante up (robbin hoodz theory)</td>\n",
       "      <td>2019</td>\n",
       "      <td>Ante Up \\nMotherfucker!\\nYeah!23Embed</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.9972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21741</th>\n",
       "      <td>21741</td>\n",
       "      <td>nine</td>\n",
       "      <td>whutcha want?</td>\n",
       "      <td>2019</td>\n",
       "      <td>Whutcha Want? Lyrics\\nI gets banned if I do ge...</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.7261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21742</th>\n",
       "      <td>21742</td>\n",
       "      <td>will smith</td>\n",
       "      <td>switch</td>\n",
       "      <td>2019</td>\n",
       "      <td>Switch Lyrics\\nYo mic check, mic check, yeah h...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21743</th>\n",
       "      <td>21743</td>\n",
       "      <td>jeezy</td>\n",
       "      <td>r.i.p.</td>\n",
       "      <td>2019</td>\n",
       "      <td>R.I.P. Lyrics\\nR.I.P we just killed the club\\n...</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21744 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0          artist                                      title  \\\n",
       "0               0   frankie laine                                  i believe   \n",
       "1               1     johnnie ray                                        cry   \n",
       "2               2     p?rez prado                                   patricia   \n",
       "3               3  lefty frizzell  if you've got the money i've got the time   \n",
       "4               4  lefty frizzell               i want to be with you always   \n",
       "...           ...             ...                                        ...   \n",
       "21739       21739         mack 10                            10 million ways   \n",
       "21740       21740          m.o.p.              ante up (robbin hoodz theory)   \n",
       "21741       21741            nine                              whutcha want?   \n",
       "21742       21742      will smith                                     switch   \n",
       "21743       21743           jeezy                                     r.i.p.   \n",
       "\n",
       "       year                                             lyrics  neg_sentiment  \\\n",
       "0      1950  I Believe Lyrics\\nI believe for every drop of ...          0.089   \n",
       "1      1950  Cry Lyrics\\nOoh-wah\\nOoh-wah\\nOoh-wah\\n\\n\\nIf ...          0.110   \n",
       "2      1950  Patricia LyricsKiss her and your lips will alw...          0.029   \n",
       "3      1950  If You’ve Got The Money I’ve Got The Time Lyri...          0.038   \n",
       "4      1950  I Want To Be With You Always Lyrics\\nI lose my...          0.074   \n",
       "...     ...                                                ...            ...   \n",
       "21739  2019  10 Million Ways Lyrics\\nCause if you fuck with...          0.133   \n",
       "21740  2019              Ante Up \\nMotherfucker!\\nYeah!23Embed          0.180   \n",
       "21741  2019  Whutcha Want? Lyrics\\nI gets banned if I do ge...          0.101   \n",
       "21742  2019  Switch Lyrics\\nYo mic check, mic check, yeah h...          0.041   \n",
       "21743  2019  R.I.P. Lyrics\\nR.I.P we just killed the club\\n...          0.193   \n",
       "\n",
       "       neu_sentiment  pos_sentiment  com_sentiment  polarity  textblob_pol  \n",
       "0              0.879          0.032        -0.7506       0.0           1.0  \n",
       "1              0.772          0.118         0.2732       1.0           1.0  \n",
       "2              0.747          0.224         0.9807       1.0           1.0  \n",
       "3              0.920          0.042         0.0112       0.0           1.0  \n",
       "4              0.702          0.224         0.9835       1.0           1.0  \n",
       "...              ...            ...            ...       ...           ...  \n",
       "21739          0.807          0.059        -0.9936       0.0           1.0  \n",
       "21740          0.730          0.090        -0.9972       0.0           0.0  \n",
       "21741          0.782          0.117        -0.7261       0.0           1.0  \n",
       "21742          0.888          0.071         0.9630       1.0           1.0  \n",
       "21743          0.762          0.046        -0.9984       0.0           1.0  \n",
       "\n",
       "[21744 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./full_lyrics_polarity_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16308,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['polarity'], test_size = 0.25, random_state = 32)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# BERT에 맞는 Tag 달아주기\n",
    "train_lyrics = []\n",
    "test_lyrics = []\n",
    "\n",
    "for i in X_train:\n",
    "  one = []\n",
    "  start = \"[CLS] \" + str(i)\n",
    "  split_lyrics = \" [SEP]\".join(start.split('\\n'))\n",
    "  split_lyrics += \" [SEP]\"\n",
    "  one.append(split_lyrics)\n",
    "  train_lyrics.append(one)\n",
    "\n",
    "# train_lyrics_df = pd.DataFrame(train_lyrics, columns=[\"lyrics_tag\"])\n",
    "\n",
    "for i in X_test:\n",
    "  one = []\n",
    "  start = \"[CLS] \" + str(i)\n",
    "  split_lyrics = \" [SEP]\".join(start.split('\\n'))\n",
    "  split_lyrics += \" [SEP]\"\n",
    "  one.append(split_lyrics)\n",
    "  test_lyrics.append(one)\n",
    "\n",
    "# test_lyrics_df = pd.DataFrame(test_lyrics, columns=[\"lyrics_tag\"])\n",
    "# train_lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
    "'''bert-base-multilingual-cased''' \n",
    "'''bert-base-uncased''' \n",
    "tokenized_data = []\n",
    "\n",
    "# 2차원 리스트로 태그를 달아주었으므로 2중 for 문 사용해서 가사에 접근\n",
    "for each_lyrics in train_lyrics:\n",
    "  for j in each_lyrics:\n",
    "    tokens = tokenizer.tokenize(j)\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "\n",
    "tokenized_test_data = []\n",
    "for each_lyrics in test_lyrics:\n",
    "  for j in each_lyrics:\n",
    "    tokens = tokenizer.tokenize(j)\n",
    "    tokenized_test_data.append(tokens)\n",
    "\n",
    "# tokenized_test_data_temp = pd.DataFrame(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = y_train.to_list()\n",
    "y_test_list = y_test.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (1.22.3)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Collecting etils[epath]\n",
      "  Using cached etils-0.7.1-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (3.19.4)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.10.0-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_datasets) (1.2.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Collecting zipp\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting importlib_resources\n",
      "  Using cached importlib_resources-5.9.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (4.3.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     -------------------------------------- 211.7/211.7 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->tensorflow_datasets) (0.4.5)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=879e097d8be35c6603b60efbd459ab45f78bdfa5c32d0f5e559033551749f4a4\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: zipp, promise, importlib_resources, googleapis-common-protos, etils, dill, tensorflow-metadata, tensorflow_datasets\n",
      "Successfully installed dill-0.3.5.1 etils-0.7.1 googleapis-common-protos-1.56.4 importlib_resources-5.9.0 promise-2.3 tensorflow-metadata-1.10.0 tensorflow_datasets-4.6.0 zipp-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = False, # add [CLS], [SEP]\n",
    "                max_length = 256, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "  \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(data):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  target_list = []\n",
    "  \n",
    "  for DATA_COL, LABEL_COL in data.to_numpy():\n",
    "    bert_input = convert_example_to_feature(DATA_COL)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    target_list.append([LABEL_COL])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, target_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'DATA_COL' : tokenized_data, 'LABEL_COL' : y_train_list})\n",
    "test = pd.DataFrame({'DATA_COL' : tokenized_test_data, 'LABEL_COL' : y_test_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "train_encoded = encode_examples(train).shuffle(100).batch(16)\n",
    "# test dataset\n",
    "test_encoded = encode_examples(test).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   1/1020 [..............................] - ETA: 7:55:19 - loss: 0.6731 - accuracy: 0.5625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\STUDY\\NLP_Toy_Proj\\modeling_bert.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/STUDY/NLP_Toy_Proj/modeling_bert.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/STUDY/NLP_Toy_Proj/modeling_bert.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/STUDY/NLP_Toy_Proj/modeling_bert.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_encoded, epochs\u001b[39m=\u001b[39;49mnumber_of_epochs, validation_data\u001b[39m=\u001b[39;49mtest_encoded)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.fit(train_encoded, epochs=number_of_epochs, validation_data=test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '''\n",
    "Jenna, time to pee on a stick\n",
    "Wh-?\n",
    "I don't wanna know\n",
    "Come one honey, you've waited long enough\n",
    "Get to it and do it!\n",
    "Wh-?\n",
    "Okay, girls!\n",
    "Enough!\n",
    "You know what we mean\n",
    "I hope you drank enough this morning\n",
    "Come on, sweetie, it's better to know\n",
    "We'll be right here with you, so\n",
    "It's no or it's yes\n",
    "But either way you gotta take the test\n",
    "A squat and a squeeze\n",
    "A prayer and a please\n",
    "It's nothing\n",
    "A stick and a line\n",
    "Just one of 'em, if i'm lucky\n",
    "A pot and a piss\n",
    "Here we go sis\n",
    "And we'll keep our focus on\n",
    "The negative!\n",
    "Read the instructions\n",
    "Se puede saber la duración de la-\n",
    "English!\n",
    "Do not insert the test stick into your vagina\n",
    "Wow\n",
    "Thank you, Dawn\n",
    "How'd I ever get myself in this mess?\n",
    "One drunk night in that stupid red dress\n",
    "Oh, I love that red dress!\n",
    "The way it sparkles\n",
    "It looks like an ice skating outfit\n",
    "Stay with us, Dawn!\n",
    "Maybe his machinery is broken somehow\n",
    "What if his boys don't swim?\n",
    "I mean, wow!\n",
    "Miraculous luck!\n",
    "Yeah, miraculous!\n",
    "To get away with an unprotected fu-\n",
    "Funny how one night can ruin you're whole life\n",
    "Don't go there yet\n",
    "We don't know what the test says\n",
    "I'm already panicked!\n",
    "Just calm down, Godammit!\n",
    "Maybe it'll all be fine\n",
    "Maybe there'll be just one line\n",
    "Come on, negative!\n",
    "I thought you don't sleep much with you husband much anymore?\n",
    "Well, I - he got me drunk\n",
    "I do stupid things when I drink\n",
    "Like sleep with my husband\n",
    "Honey, we've all made that mistake\n",
    "Focus on the negative!\n",
    "This will all be alright\n",
    "It was only one night\n",
    "Someone send me a sign\n",
    "One line\n",
    "One line\n",
    "That means the test is ready\n",
    "This is it\n",
    "Shit!\n",
    "'''\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "\n",
    "truncation=True,\n",
    "\n",
    "padding=True,\n",
    "\n",
    "return_tensors=\"tf\")\n",
    "tf_output = model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]],\": {:.2f}%\".format(abs(tf_output.take(label[0])) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
